### GENETAL

Vulkan - язык приказов, который CPU использует, чтобы управлять GPU.
Vulkan — это очень строгий и детальный протокол, который описывает, как CPU должен подготовить данные, 
упаковать инструкции и отправить их на выполнение в GPU

### Rendering Pipeline

Это "сборочный конвейер" внутри GPU. Когда вы даете команду нарисовать треугольник, его данные (координаты вершин) проходят через несколько обязательных этапов:
    - Vertex input (Входные данные вершин)
    - Vertex Shader (Вершинный буфер)
    - Rasterization (Растеризация)
    - Fragment Shader (Фрагментный (пиксельный) шейдер)
    - Blending (Смешивание)

1. Vertex input
   GPU нужно получить сырые данные о моей модели из памяти. 
   В initialize я создаю plabe_mesh.vertex_buffer и cube_mesh.vertex_buffer. Это буферы в VRAM (память в GPU),
   куда я копирую данные из векторов vertices. Структура Vertex описывает, что лежит в этом буфере для каждой
   вершины.
2. Vertex Shader
   Маленькая программа, которая выполняется GPU для каждой вершины индивидуально. Задача - взять позицию
   вершины в локальных координатах модели и вычислить ее финальную позицию на экране.
   У меня это происходит в shader.verts
3. Rasterization
   GPU берет 3 обработанные вершины, которые образуют треугольник, и определяет, какие пиксели на экране попадают внутрь. На этом этапе он также интерполирует (плавно изменяет) данные, которые мы получили от вершинного шейдера (например, нормаль f_normal) для каждого пикселя. а этом этапе он также интерполирует (плавно изменяет) данные, которые мы получили от вершинного шейдера (например, нормаль f_normal) для каждого пикселя. Если у одной вершины нормаль смотрит вверх, а у другой вбок, то у пикселя посередине нормаль будет смотреть по диагонали.
   У меня настраивается в initialize через VkPipelineRasterizationStateCreateInfo.
4. Fragment Shader
   Вторая маленькая программа. Выполняется для каждого пикселя, который попал внутрь треугольника на этапе растеризации. Задача - определить, каким цветом будет пиксель. Здесь же происходит "магия" освещения.
   У меня это shader.frag.
5. Blending
   Последний этап. Если пиксель уже чем-то закрашен (например, за треугольником есть другой), GPU решает, что с этим делать. Смешать цвета (для прозрачности) или просто переписать старый цвет новым.
   У меня в коде настраивается в VkPipelineColorBlendStateCreateInfo.


### Передача данных в GPU

1. Vertex Buffers
   Это большие объемы данных, описывающие геометрию. Позиции, нормали, UV-координаты.
   В моем коде plane_mesh.vertex_buffer. Обычно загружаются в VRAM 1 раз и больше не меняются.
2. Uniform Buffers
   Небольшие объемы данных, которые являются общими для целого вызова отрисовки (матрица проекции или позиция камеры). В моем коде: scene_uniforms_buffer (хранит view_projection) - обновляется один раз за кадр; model_uniforms_buffer (хранит model, albedo_color) - Dynamic UBO. Выделяю память под max_models штук при рисовке просто указываю смещение.
3. Storage Buffers
   Похожи на UBO, но могут быть гораздо большего размера и позволяют запись из шейдера (хотя у меня readonly). Идеально подходят для массивов данных переменного размера. В моем коде light_ssbo_buffer, я храню в нем массив PointLight и их количество. Это главное требование. 
4. Push Constants
   Самый быстрый способ передать крошечный объем данных (обычно не более 128-256 байт). Они передаются вместе с самой командой отрисовки, без использования отдельных буферов. В моем коде vkCmdPushConstants. Вы передаете в них все данные об окружающем, направленном и прожекторном свете, а также позицию камеры. Это очень грамотное решение, так как эти данные общие для всех объектов в кадре.

Кратко:
- Вершины -> Vertex Buffer
- Глобальные данные сцены (матрицы) -> UBO
- Массив чего-либо (источники света) -> SSBO
- Очень маленькие, часто меняющиеся данные -> Push Constants

### Командные буферы и дескрипторы

Командный буфер - приказ для GPU что-то делать. 
- render(VkCommandBuffer cmd, ...) - эта функция ничего не рисует, она записывает последовательность команд в cmd.
- vkBeginCommandBuffer, vkCmd..., vkEndCommandBuffer - процесс записи
- Только когда CPU отправляет этот командный буфер в очередь GPU (vkQueueSubmit, это делает veekay под капотом)

А как шейдер узнает, откуда брать данные для UBO и SSBO? Через дескрипторы.
- VkDescriptorSet - таблица ссылок.
  В шейдере я: layout(set=0, binding=2) buffer LightSSBO, а на CPU говорю: "В set=0 на binding=2 положи, пожалуйста, ссылку на буфер light_ssbo_buffer"
- vkCmdBindDescriptorSets - говорим GPU: "Для следующих команд отрисовки используй вот это оглавление (descriptor_set)"

### Возможные вопросы и ответы к ним

В: Что такое рендер-пасс (VkRenderPass)?
О: Рендер-пасс описывает, с какими изображениями (attachments) мы работаем во время рендеринга. В вашем случае это два изображения: цветовой буфер (куда мы рисуем) и буфер глубины (для Z-теста, чтобы ближние объекты перекрывали дальние). Команда vkCmdBeginRenderPass говорит GPU "начать работу с этими изображениями, очистив их заданными цветами".

В: Объясните вашу реализацию камеры. В чем разница между двумя режимами?
О: У меня реализовано два режима.
    - Трансформационная модель: Камера рассматривается как объект в мире. У нее есть позиция (position) и углы Эйлера (rotation). Матрица вида (view) вычисляется как обратная трансформация камеры: сначала поворот в обратную сторону (-rotation), потом перенос в обратную сторону (-position). Это интуитивно понятно для управления "от первого лица".
    - Режим Look-At: Этот режим более математический. Мы задаем три вектора: позицию камеры (position), точку, куда смотреть (target), и вектор "верха" (обычно (0,1,0)). Функция look_at строит ортонормированный базис (систему координат) для камеры и вычисляет матрицу вида. Этот режим удобен для наблюдения за объектом со стороны. Переключение реализовано через флаг is_look_at.
  
В: Как работает модель освещения Блинн-Фонга? Чем она отличается от Фонга?
О: Модель Блинн-Фонга рассчитывает три компонента света:
    - Ambient (Рассеянный): Постоянный свет, который подсвечивает все поверхности одинаково, имитируя многократные отражения света в сцене.
    - Diffuse (Диффузный): Имитирует отражение света от матовых поверхностей. Его интенсивность зависит от угла между нормалью к поверхности и направлением на свет (dot(N, L)). Чем больше поверхность повернута к свету, тем она ярче.
    - Specular (Бликовый): Имитирует блики на глянцевых поверхностях. Ключевое отличие от Фонга: вместо вычисления отраженного от света вектора, мы вычисляем "вектор полупути" (H = normalize(L + V)), который лежит ровно посередине между направлением на свет (L) и направлением на камеру (V). Блик максимален, когда этот вектор H совпадает с нормалью N. Считается, что это дает более мягкие и физически правдоподобные блики и работает быстрее.
  
В: Как вы реализовали плавные края у прожектора?
О: Я использую два угла для конуса прожектора: внутренний (inner_cutOff) и внешний (outer_cutOff).
1. Я вычисляю косинус угла между вектором от пикселя к прожектору и направлением самого прожектора.
2. Если этот косинус меньше косинуса внешнего угла, пиксель находится вне света (интенсивность 0).
3. Если косинус больше косинуса внутреннего угла, пиксель полностью освещен (интенсивность 1).
4. Если косинус находится между этими двумя значениями, я плавно интерполирую интенсивность от 0 до 1. Это создает эффект мягкой полутени (penumbra) по краю светового пятна. В шейдер я передаю уже предрассчитанные косинусы углов, чтобы не вычислять cos() для каждого пикселя.

В (сложный вопрос): В вашем вершинном шейдере вы трансформируете нормаль как model * vec4(v_normal, 0.0f). В каком случае это приведет к неверному результату?
О: Этот способ работает корректно только для вращения и равномерного масштабирования. Если я применю неравномерное масштабирование (например, сплющу объект по одной из осей), нормали перестанут быть перпендикулярны поверхности. Правильный способ — использовать обратную транспонированную матрицу модели (transpose(inverse(model))) для трансформации нормалей.


### ПРОСТО

Копирование: Поскольку CPU не может напрямую писать в быструю DEVICE_LOCAL VRAM, происходит промежуточный шаг (который veekay прячет):
Создается временный "staging" буфер в памяти, видимой и CPU, и GPU (HOST_VISIBLE).
CPU копирует данные из vertices.data() в этот staging буфер.
В командный буфер записывается команда vkCmdCopyBuffer, которая приказывает GPU скопировать данные из staging буфера в финальный vertex_buffer.
Staging буфер уничтожается.

ИТОГ ПУТИ:
CPU:
1. Создаем Model с material.albedo = red.
2. В update() копируем red в ModelUniforms.
3. Копируем ModelUniforms в model_uniforms_buffer со смещением offset.
4. В render() отдаем команду vkCmdBindDescriptorSets с тем же offset.
5. Отдаем команду vkCmdDrawIndexed.
GPU (Шейдеры):
6. shader.frag запускается для пикселя.
7. Он читает uniform ModelUniforms. GPU, зная offset, находит в памяти и считывает red.
8. red используется в формуле освещения.
9. Итоговый цвет записывается на экран.





// описываем геометрию объекта	
struct Mesh {
	veekay::graphics::Buffer* vertex_buffer; // Указатель на буфер в VRAM, где лежат координаты, нормали и UV всех вершин (углов) объекта.
	veekay::graphics::Buffer* index_buffer; // Указатель на буфер, который говорит, в каком порядке соединять вершины, чтобы получить треугольники.
	uint32_t indices; // Количество индексов. Нужно для команды отрисовки.
};

Пример: cube_mesh и plane_mesh — это два разных экземпляра этой структуры, описывающие геометрию куба и плоскости.

// Внешний вид поверхности объекта
struct Material {
	veekay::vec3 albedo = {1.0f, 1.0f, 1.0f}; // основной цвет объекта
	float _pad0;
	veekay::vec3 specular = {1.0f, 1.0f, 1.0f}; // цвет блика
	float shininess = 32.0f; // чем больше, тем меньше и ярче блик
};

// Как данные об одном объекте лежат в GPU
struct ModelUniforms {
	veekay::mat4 model; //Матрица модели. Отвечает за положение, поворот и размер этого конкретного объекта в сцене.
   // поля, куда копируем данные из Material
	veekay::vec3 albedo_color;
	float shininess;
	veekay::vec3 specular_color;
	float _pad;
};
Ключевое отличие от Material: ModelUniforms — это "транспортная" структура для передачи данных в GPU, она должна быть совместима с правилами выравнивания памяти в шейдерах (поэтому там есть _pad). Material — это просто удобная C++ структура для хранения этих данных на стороне CPU.

// описываем один полноценный объект
struct Model {
	Mesh mesh; // "Какую геометрию использовать?" (например, cube_mesh).
	Transform transform; // "Где в мире этот объект находится, как он повернут и какого он размера?". Из transform вычисляется матрица model для ModelUniforms.
	Material material; // "Как выглядит поверхность этого объекта?" (его цвет, блеск и т.д.).
};

veekay::mat4 view() const; // вычисляем и возвращаем матрицу вида
Матрица вида преобразует координаты из мирового пространства в пространство камеры. Проще говоря, она "устанавливает камеру" в определенной точке и поворачивает ее в нужном направлении. Она делает так, чтобы мир "двигался" обратно движению камеры.
Как работает:
   Внутри она проверяет флаг is_look_at и либо вычисляет матрицу на основе position и rotation (твой вариант 1), либо на основе position, target и up (доп. задание).

veekay::mat4 view_projection(float aspect_ratio) const; // матрица вида * матрицу проекции
Матрица проекции "сплющивает" 3D-мир в 2D-изображение, создавая эффект перспективы (дальние объекты меньше). Объединение View и Projection в одну матрицу — это стандартная оптимизация. Результат этой функции мы записываем в SceneUniforms и отправляем в шейдер.


struct LightSSBO {
	PointLight point_lights[max_point_lights];
	uint32_t point_light_count = 0;
	veekay::vec3 _pad[3];
};
Что это: C++ структура, которая точно повторяет структуру данных в шейдере для SSBO (Shader Storage Buffer Object). Это наш "контейнер" для всех точечных источников света, который мы целиком скопируем в память GPU.


// обертка вокруг SPIR-V кода. Просто ресурс, который подключаю к конвейеру
	VkShaderModule vertex_shader_module;
Что это: "Ручка" к скомпилированному коду шейдера (SPIR-V), загруженному в память.
Аналогия: Это как exe-файл для твоей программы. Он сам по себе не работает, пока ты его не запустишь. vertex_shader_module — это exe для вершинного шейдера, fragment_shader_module — для фрагментного.

// описание интерфейса между шейдерами и данными, передаваемыми из кода
	VkPipelineLayout pipeline_layout;
Что он описывает: Он описывает ДВЕ вещи:
   - Какие Descriptor Sets будут использоваться. Он хранит в себе указатель на descriptor_set_layout. Это говорит конвейеру: "Будь готов принять один Descriptor Set, который соответствует вот этому шаблону (с binding 0 для UBO, 1 для Dynamic UBO, 2 для SSBO)".
   - Какие Push Constants будут использоваться. Он описывает, сколько байт push-констант вы будете передавать и в каких шейдерах они будут доступны (stageFlags = VK_SHADER_STAGE_FRAGMENT_BIT).
Зачем он нужен: Чтобы Vulkan мог заранее проверить, что данные, которые вы пытаетесь привязать (vkCmdBindDescriptorSets) или передать (vkCmdPushConstants), соответствуют тому, чего ожидают шейдеры. Это строгий контракт. VkPipeline (следующий пункт) не может быть создан без VkPipelineLayout.


// сконфигурированный графический конвейер
	VkPipeline pipeline;
Что он "запекает" в себе: Всё. Вообще всё, что касается состояния GPU во время отрисовки:
   - Шейдеры: Какие VkShaderModule использовать для вершинной и фрагментной стадий.
   - Формат входных данных: Как интерпретировать данные из вершинного буфера (структура Vertex).
   - Способ сборки: Что вершины образуют треугольники (VK_PRIMITIVE_TOPOLOGY_TRIANGLE_LIST).
   - Состояние растеризатора: Заливать треугольники сплошным цветом, отбрасывать задние грани.
   - Состояние теста глубины: Включен ли Z-тест, как он сравнивает глубину.
   - Состояние смешивания цветов: Как цвет из фрагментного шейдера смешивается с тем, что уже есть в пикселе.
   - Интерфейс: Ссылка на pipeline_layout, который мы разобрали выше.
Это цель всей функции initialize. Вы создаете этот гигантский объект состояния один раз при запуске, чтобы потом в цикле рендеринга просто сказать GPU: "Используй вот этот конвейер" (vkCmdBindPipeline). Это невероятно эффективно, потому что драйверу не нужно каждый кадр заново решать, как ему рисовать треугольники. Он просто активирует уже готовую, скомпилированную "программу" для GPU.


VkShaderModule result;
	// veekay::app.vk_device - логическое устройство GPU
	// &info - указатель на VkShaderModuleCreateInfo (где инфа о шейдере)
	// nullptr - указатель на аллокатор памяти (используем базовый)
	if (vkCreateShaderModule(veekay::app.vk_device, &info, nullptr, &result) != VK_SUCCESS) {
		return nullptr;
	}
Что делает функция:
   - Она берет информацию из info.
   - Проверяет байт-код шейдера на корректность (является ли он валидным SPIR-V).
   - Создает внутри драйвера, в памяти GPU, внутренний объект, представляющий этот скомпилированный шейдер.
   - Записывает в нашу переменную result (по переданному адресу &result) уникальный идентификатор (дескриптор) этого созданного объекта.

мультисемплинг - Это техника сглаживания "лесенек" на краях объектов. Вместо того, чтобы считать цвет для одного центрального сэмпла (точки) внутри пикселя, GPU считает его для нескольких сэмплов (2x, 4x, 8x), а потом усредняет результат. Это делает края объектов более гладкими.


Descriptor Set — это просто массив адресов памяти GPU. Он нужен, чтобы отделить "что рисовать" (vkCmdDraw) от "с какими данными" (vkCmdBindDescriptorSets). Это позволяет GPU быть очень эффективным: он может получить один приказ "используй вот этот набор данных" и потом выполнить 1000 команд vkCmdDraw с этим набором, не тратя время на поиск адресов для каждого объекта.




VkPipelineLayout — это полный "контракт" или "API" ваших шейдеров.
Он формально описывает все внешние данные, которые шейдеры могут запросить.
Этот объект нужен в двух местах:
   - При создании VkPipeline: Графический конвейер "запекает" в себе этот layout. Это позволяет драйверу скомпилировать и оптимизировать шейдеры, точно зная, откуда они будут брать данные.
   - При записи команд в VkCommandBuffer:
      - Когда ты вызываешь vkCmdBindDescriptorSets(..., pipeline_layout, ...)
      - Когда ты вызываешь vkCmdPushConstants(..., pipeline_layout, ...)
      - Ты передаешь pipeline_layout в эти функции. Vulkan использует его для проверки: "Действительно ли в layout'е для set=0 есть binding=2? Действительно ли layout разрешает передавать 128 байт push-констант во фрагментный шейдер?". Это обеспечивает строгий контроль и предотвращает ошибки.




Графический конвейер — это последовательность программируемых и фиксированных этапов на GPU, которая преобразует набор математических описаний (вершины, треугольники) в двумерное изображение (массив пикселей).
Конвейер выглядит так:
   - Входные данные (Input Assembler): GPU читает твои cube_mesh.vertex_buffer и cube_mesh.index_buffer, чтобы собрать данные для треугольников.
   - Вершинный шейдер (Vertex Shader): Запускается твой shader.vert. Он получает v_position и v_normal, вычисляет gl_Position и передает f_position и f_normal дальше.
   - Растеризация (Rasterization): Непрограммируемый этап. GPU:
       - Берет три gl_Position и определяет, какие пиксели экрана покрыты треугольником.
       - Для каждого такого пикселя выполняет барицентрическую интерполяцию для f_position и f_normal.
   - Фрагментный шейдер (Fragment Shader): Запускается твой shader.frag для каждого пикселя. Он получает интерполированные f_position и f_normal. Используя их, а также данные из uniform-буферов (model, pc) и SSBO (lights), он вычисляет final_color.
   - Выходной этап (Output Merger): GPU берет final_color, выполняет тест глубины и записывает итоговый цвет в framebuffer, который ты видишь на экране.




Текстурирование в графическом конвейере.
1. На вход подается зеленый треугольник. Это твои "сырые" данные — координаты вершин из vertex_buffer.
2. Vertex Shader (Вершинный шейдер):
   1. Что делает: Принимает вершины треугольника и изменяет их положение. На картинке видно, что вершины выходного треугольника сместились.
   2. Как это связано с твоим кодом: Это в точности твой файл shader.vert. Его главная задача — взять входную позицию v_position и вычислить финальную позицию gl_Position.
3. Rasterizer (Растеризатор):
   1. Что делает: Берет три обработанные вершины и определяет, какие пиксели на экране попадают внутрь этого треугольника. Он превращает векторную фигуру (треугольник) в набор пикселей (фрагментов). На картинке это показано как заштрихованная сетка пикселей. Как это связано с твоим кодом: Это аппаратный этап GPU. Ты не пишешь для него код, но ты его настраиваешь в main.cpp через структуру VkPipelineRasterizationStateCreateInfo (где ты указываешь polygonMode = VK_POLYGON_MODE_FILL).
4. Fragment Shader (Фрагментный шейдер):
   1. Что делает: Выполняется для каждого пикселя, найденного растеризатором. Его задача — определить финальный цвет этого пикселя. На картинке показано, что он берет данные от растеризатора, берет текстуру кирпича и на выходе выдает окрашенный пиксель (в итоге получается чайник с текстурой кирпича).
   2. Как это связано с твоим кодом: Это в точности твой файл shader.frag. Его работа — вычислить final_color для каждого пикселя, используя данные о нормалях, материалах и источниках света.


Растеризация
Про растеризацию
Цель растеризации: Взять три вершины, образующие треугольник в пространстве экрана, и сгенерировать для фрагментного шейдера список всех пикселей, которые этот треугольник покрывает.

Шаг 1: Настройка треугольника (Triangle Setup)
Вход: Растеризатор получает три обработанные вершины из твоего shader.vert. Для каждой вершины он знает:
Ее финальные экранные координаты (из gl_Position).
Значения всех out переменных (f_position, f_normal, f_uv).
Действие: GPU вычисляет "дельта-значения" и уравнения ребер треугольника. По сути, он математически описывает три линии, образующие стороны треугольника. Это подготовка для следующего шага.

Шаг 2: Сканирование (Scan Conversion)
Задача: Найти все пиксели, которые могут принадлежать треугольнику.
Действие:
- GPU определяет ограничивающий прямоугольник (bounding box) вокруг треугольника. Это самый маленький прямоугольник, в который полностью вписывается треугольник.
- GPU не будет проверять все пиксели экрана. Он будет итерировать только по тем пикселям, центры которых находятся внутри этого прямоугольника. Это огромная оптимизация.

Шаг 3: Тест "Внутри/Снаружи" (Inside/Outside Test)
Задача: Для каждого пикселя из bounding box'а определить, находится ли он действительно внутри треугольника.
Действие:
Для центра каждого пикселя GPU выполняет тест покрытия. Самый распространенный метод — "edge functions".
- Представь, что каждая сторона треугольника — это бесконечная линия, которая делит экран на две полуплоскости.
Для каждой из трех сторон GPU определяет, находится ли центр пикселя "справа" или "слева" от этой линии (математически это делается с помощью знака результата векторного произведения).
- Если центр пикселя находится с "правильной" стороны от всех трех линий одновременно, то он считается покрытым треугольником. Если хотя бы для одной линии он с "неправильной" стороны — пиксель отбрасывается.

Шаг 4: Интерполяция атрибутов (Attribute Interpolation)
Это самый важный шаг, который объясняет картинку с формулой.
Задача: Для каждого пикселя, который прошел тест "внутри", вычислить значения атрибутов (f_position, f_normal, f_uv), которые будут переданы во фрагментный шейдер.
Действие:
- Для этого пикселя GPU вычисляет его барицентрические координаты (α, β, γ).
   - Это три "веса", которые показывают, насколько пиксель близок к каждой из трех вершин треугольника (v₀, v₁, v₂).
   - Если пиксель находится точно на вершине v₀, его координаты будут {α=1, β=0, γ=0}.
   - Если пиксель находится ровно посередине ребра между v₀ и v₁, его координаты {α=0.5, β=0.5, γ=0}.
   - Сумма α + β + γ всегда равна 1.
- GPU использует эти веса для линейной интерполяции всех out переменных из вершинного шейдера, используя ту самую формулу с твоей картинки: u = α u₀ + β u₁ + γ u₂.

Применительно к твоему коду:
   - interpolated_f_normal = α * f_normal_at_v0 + β * f_normal_at_v1 + γ * f_normal_at_v2;
   - interpolated_f_uv = α * f_uv_at_v0 + β * f_uv_at_v1 + γ * f_uv_at_v2;
Результат:
Для каждого покрытого пикселя генерируется фрагмент. Фрагмент — это, по сути, "кандидат в пиксели", который содержит:
   - Координаты пикселя на экране.
   - Интерполированные значения всех атрибутов (f_position, f_normal и т.д.).


Сэмплирование (от англ. sample — образец, проба) — это процесс чтения/извлечения значения (чаще всего цвета) из текстуры по заданным координатам.
Проблема, которую решает сэмплирование
   1. Текстура — это просто 2D-массив пикселей (текселей). Это дискретный набор данных. У нее есть цвет в точке {0,0}, {1,0}, {2,0}, но нет никакого цвета в точке {1.5, 2.8}.
   2. Поверхность 3D-модели — это математически непрерывная поверхность.
   3. UV-координаты, которые приходят в твой фрагментный шейдер (in vec2 f_uv), являются непрерывными. После интерполяции растеризатором они могут принять любое значение, например, {0.521, 0.874}.

Проблема: Как получить цвет из дискретной сетки (текстуры) по непрерывным координатам {0.521, 0.874}?
Сэмплирование — это не просто "взять пиксель". Это процесс, который настраивается и выполняется аппаратно на GPU. Он состоит из двух главных частей, которые настраиваются в объекте VkSampler.
1. Фильтрация (Filtering) — Ответ на вопрос "Что делать, если мы попали МЕЖДУ пикселями?"
   Когда твоя UV-координата {0.521, 0.874} (в масштабе текстуры) попадает не точно в центр одного текселя, а между ними, GPU должен решить, какой цвет вернуть. Есть два основных способа:
      - VK_FILTER_NEAREST (Фильтрация по ближайшему соседу):
      Как работает: GPU просто находит один, ближайший к координате {0.521, 0.874}, центр текселя и возвращает его цвет.
      Результат: Изображение выглядит "пиксельным", "блочным", особенно при увеличении. Хорошо для ретро-игр, плохо для реализма.
      - VK_FILTER_LINEAR (основа билинейной фильтрации):
      Как работает: GPU находит четыре ближайших текселя, окружающих координату {0.521, 0.874}. Затем он вычисляет взвешенное среднее их цветов. Вес каждого из четырех цветов зависит от того, насколько близко к нему находится точка сэмплирования.
      Результат: Плавный, немного "размытый" переход цвета. Гораздо лучше подходит для реалистичных текстур, так как скрывает пиксельную сетку. Это стандарт для современной графики.
2. Адресация (Addressing / Wrapping) — Ответ на вопрос "Что делать, если мы вышли ЗА ПРЕДЕЛЫ текстуры?"
   UV-координаты не обязаны быть в диапазоне от 0.0 до 1.0. Они могут быть, например, 2.5 или -0.2. Режим адресации говорит GPU, что делать в этом случае.
      - VK_SAMPLER_ADDRESS_MODE_REPEAT (Повторение):
      Как работает: Текстура "зацикливается". Координата 1.2 будет обработана как 0.2, 2.5 как 0.5, -0.2 как 0.8.
      Результат: Текстура бесконечно повторяется (тайлится). Идеально для полов, кирпичных стен, обоев.
      - VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_EDGE (Прижатие к краю):
      Как работает: Любая координата больше 1.0 считается равной 1.0. Любая меньше 0.0 считается равной 0.0.
      Результат: Крайние пиксели текстуры "растягиваются" до бесконечности. Это полезно, чтобы избежать артефактов "заворачивания" по краям объектов, которые не должны тайлиться (например, текстура лица персонажа).
      В твоем коде: Именно этот режим ты установил для missing_texture_sampler.
      - Mirroded repeat
      - Clamp to Border

Алиасинг - вид сэмплирования, при котором пиксель может занимать несколько текселей. В таком случае мы выбираем 1 или смешиваем несколько текселей.

Mip-Maps
Проблема: Алиасинг и Рябь
- "Пиксель может занимать много текселей"
      - Что это значит: Представь себе стену с текстурой кирпича размером 1024x1024 пикселей. Когда ты стоишь к ней вплотную, один пиксель на твоем экране примерно соответствует одному текселю (пикселю текстуры). Но когда ты отходишь очень далеко, вся эта стена на экране может занимать, например, всего 10x10 пикселей. Это значит, что один пиксель на экране теперь должен как-то отобразить информацию из огромной области текстуры размером примерно 100x100 текселей.
- "Поскольку дальние объекты занимают меньше пикселей, но происходит сэмплирование текселя всего изображения, то возникает рябь"
     - Что это значит: Когда GPU пытается сэмплировать цвет для этого одного экранного пикселя, он использует UV-координату и обычную билинейную фильтрацию. Он возьмет 4 ближайших текселя из огромной, детализированной текстуры 1024x1024 и смешает их.
     - Проблема: При малейшем движении камеры UV-координата для этого пикселя немного сместится, и GPU захватит совершенно другую четверку текселей. Если на текстуре есть мелкие, контрастные детали (как швы между кирпичами), то в одном кадре пиксель может стать красным (попал в кирпич), а в следующем — серым (попал в шов).
     - Результат: При движении или даже просто при рендеринге статичной картинки удаленные текстуры начинают мерцать, искриться и "шуметь". Это и называется рябь (moiré) или алиасинг сэмплирования.
Решение: Mip-maps
- "Для решения этой проблемы, нужно сделать так, чтобы GPU выбирал меньшие пре-фильтрованные версии изображения — мип-мапы"
      - Что такое мип-мап: Это заранее созданная цепочка уменьшенных копий оригинальной текстуры.
      - Например, если у тебя есть текстура 1024x1024 (уровень 0), то мип-мапы для нее будут:
         - Уровень 1: 512x512
         - Уровень 2: 256x256
         - Уровень 3: 128x128
         - ... и так далее до 1x1.
      - "Пре-фильтрованные" — это ключевой момент. Каждый пиксель в мип-уровне 512x512 — это усредненный цвет 4-х соответствующих пикселей из уровня 1024x1024. То есть, они уже содержат "обобщенную" информацию о цвете.
- Как это решает проблему:
      - Когда GPU рендерит далекий объект, он автоматически определяет, что для одного пикселя экрана нужно покрыть большую область текселей.
      - Вместо того чтобы сэмплировать из огромной и детализированной текстуры 1024x1024, он выбирает подходящий, уже уменьшенный мип-уровень (например, 128x128).
      - На этом маленьком мип-уровне одному пикселю экрана будет соответствовать уже гораздо меньшее количество текселей (например, 1-2).
      - Результат: Рябь исчезает, потому что GPU работает с версией текстуры, которая уже соответствует масштабу объекта на экране. Переходы между мип-уровнями также могут быть сглажены (это называется трилинейная фильтрация).
Правила генерации
- "Каждый мип-мап в два раза меньше по размеру, чем предыдущий": Это стандартное правило.
- "Для текстуры с размерами степени двойки...": Если размеры текстуры — это степени двойки (например, 256, 512, 1024), то процесс генерации мип-мапов тривиален и идет до конца (до 1x1 пикселя).
- "Если изображение имеет размер не степени двойки...": Хотя современные GPU умеют работать с такими текстурами, исторически и для максимальной совместимости и производительности рекомендуется использовать текстуры с размерами, являющимися степенями двойки (NPOT — Non-Power-of-Two текстуры могут иметь ограничения). На слайде указано, что для них мип-мапы не генерируют, что является упрощением — их можно сгенерировать, но это может быть менее эффективно.

Анизотропная фильтрация решает проблему, с которой Mip-mapping справляется плохо.
Проблема: Текстуры под острым углом
   - Представь, что ты стоишь на очень длинной дороге, уходящей вдаль.
   - Ближняя часть дороги: Текстура асфальта выглядит четко. Один пиксель экрана покрывает примерно квадратную область текстуры.
   - Дальняя часть дороги: Текстура сильно искажается перспективой. Она "сплющивается" по вертикали. Теперь один пиксель экрана покрывает область текстуры, которая является не квадратом, а очень длинным и узким прямоугольником.
Почему Mip-mapping здесь не справляется?
- Mip-mapping выбирает уровень детализации (мип-уровень), исходя из того, что пиксель покрывает примерно квадратную область.
- Для нашего "прямоугольника" на дальней дороге GPU оказывается перед выбором:
   - Выбрать детализированный мип-уровень (например, 1024x1024), чтобы сохранить четкость по горизонтали. Но тогда по вертикали ему придется пропустить кучу текселей, что вызовет рябь и алиасинг.
   - Выбрать размытый мип-уровень (например, 128x128), чтобы убрать рябь по вертикали. Но тогда текстура станет слишком размытой и потеряет всю детализацию по горизонтали.
Итог: С обычной билинейной/трилинейной фильтрацией текстуры под острым углом (полы, потолки, дороги) выглядят либо "шумными", либо "замыленными".
Трилинейная фильтрация - комбинация VK_FILTER_LINEAR и мип-мапов
      Представь, что у тебя есть текстура с мип-уровнями:
         mip_level_2 (например, 256x256)
         mip_level_3 (например, 128x128)
      GPU определил, что объект находится на расстоянии, которое "между" этими двумя уровнями.
      Вот что делает трилинейная фильтрация:
         Первая билинейная фильтрация:
            Берется mip_level_2.
            На нем выполняется VK_FILTER_LINEAR (смешиваются 4 текселя).
            Получаем Цвет_А.
         Вторая билинейная фильтрация:
            Берется mip_level_3.
            На нем снова выполняется VK_FILTER_LINEAR (смешиваются еще 4 текселя).
            Получаем Цвет_Б.
         Третья, финальная линейная интерполяция:
            Теперь GPU смешивает Цвет_А и Цвет_Б.
            Пропорция смешивания зависит от того, насколько расстояние до объекта ближе к "идеальному" для mip_level_2 или mip_level_3.

Решение: Анизотропная фильтрация
"Анизотропия" означает "имеющий разные свойства в разных направлениях". В отличие от Mip-mapping, который считает, что область сэмплирования всегда квадратная (изотропная), анизотропная фильтрация учитывает, что она может быть вытянутой.
Как она работает:
- Определение формы: GPU определяет, что пиксель на экране покрывает не квадрат, а вытянутый прямоугольник на текстуре. Он также определяет ориентацию этого прямоугольника.
- Множественные сэмплы: Вместо того чтобы брать 4 текселя (как билинейная) или 8 (как трилинейная), анизотропная фильтрация берет несколько сэмплов вдоль длинной оси этого прямоугольника.
Уровень анизотропии (2x, 4x, 8x, 16x): Это число (которое ты настраиваешь в игре) определяет, максимум сколько сэмплов GPU может взять вдоль этой оси.
   - 2x: Берется 2 сэмпла. Улучшает качество, но не сильно.
   - 16x: Берется до 16 сэмплов. Значительно улучшает качество текстур под экстремальными углами.
- Усреднение: Цвета всех взятых сэмплов (которые могут быть взяты из разных мип-уровней) грамотно усредняются.
Результат: Текстуры, расположенные под острым углом к камере, остаются четкими и детализированными, но при этом без ряби и мерцания.


Модель Блинн-Фонга
Модель освещения Блинн-Фонга — это математическая модель (упрощение реального мира), которая позволяет быстро и достаточно реалистично рассчитать, как свет взаимодействует с поверхностью объекта.

Ее цель — определить цвет пикселя на поверхности объекта, основываясь на:
1. Свойствах самой поверхности (ее цвет, глянцевость).
2. Положении источника света.
3. Положении наблюдателя (камеры).

Модель Блинн-Фонга (как и модель Фонга) говорит, что итоговый цвет, который мы видим, — это сумма трех разных типов отраженного света.
1. Ambient (Рассеянный/Фоновый компонент)
   - Идея: В реальном мире свет отражается от всех поверхностей, стен, потолка и т.д. В результате даже те участки объекта, на которые не падает прямой свет, не являются абсолютно черными. Они слабо освещены этим "вторичным" светом.
   - Как считается: Это самый простой компонент. Мы просто берем некий "цвет окружения" (ambient_color) и умножаем его на основной цвет объекта (albedo_color). Этот компонент не зависит ни от положения света, ни от положения камеры. Он просто дает базовую, минимальную освещенность.
   - Зачем нужен: Чтобы тени не были абсолютно черными "дырами", а имели цвет.
2. Diffuse (Диффузный компонент)
   - Идея: Имитирует отражение света от матовых поверхностей (как бумага, матовая краска, мел). Такие поверхности рассеивают свет одинаково во все стороны.
   - Как работает: Яркость диффузного отражения зависит только от одного фактора — угла падения света.
      - Если свет падает на поверхность под прямым углом (перпендикулярно), она освещена максимально ярко.
      - Если свет падает по касательной, она освещена очень слабо.
      - Если свет падает "сзади" (с другой стороны поверхности), она не освещена совсем.
   - Как считается: Мы вычисляем косинус угла между вектором нормали к поверхности (N) и вектором, направленным к источнику света (L). Этот косинус — dot(N, L). Чем он больше (ближе к 1), тем ярче диффузный компонент.
   - Зачем нужен: Создает основной объем и форму объекта, показывая, какие его части повернуты к свету, а какие — нет.
3. Specular (Бликовый/Зеркальный компонент)
   - Идея: Имитирует отражение света от глянцевых поверхностей (как пластик, полированный металл). Такие поверхности отражают свет не во все стороны, а в основном в одном, "зеркальном" направлении. Это и создает блик.
   - Как работает: Блик виден только тогда, когда наблюдатель (камера) находится в правильном месте, чтобы "поймать" этот отраженный луч.
   - Как считается (и в чем отличие от Фонга):
      - Модель Фонга: Сначала вычисляла вектор света, идеально отраженный от поверхности, а потом смотрела, насколько этот отраженный вектор близок к вектору взгляда (V). Это работало, но было не очень эффективно и иногда давало не совсем естественные блики.
      - Модель Блинн-Фонга (твоя модель): Вместо этого вычисляется "вектор полупути" (H). Это вектор, который лежит ровно посередине между вектором к свету (L) и вектором к наблюдателю (V).
      - Логика: Блик будет максимальным, когда этот "вектор полупути" H совпадает с нормалью к поверхности N. Это означает, что нормаль идеально "отражает" свет в глаз наблюдателя.
      - Мы вычисляем косинус угла между N и H (dot(N, H)). Чем он больше, тем интенсивнее блик.
      - Shininess (Блескучесть): Этот косинус возводится в степень shininess. Большое значение shininess (например, 256) означает, что даже небольшое отклонение H от N резко уменьшит яркость блика. Результат — маленький, резкий, яркий блик (как на мокром пластике). Маленькое значение shininess (например, 8) дает большой, размытый блик (как на матовом пластике).
   - Зачем нужен: Чтобы передать материал поверхности, показать, насколько он глянцевый.

Модель Блинн-Фонга — это быстрый способ сымитировать реальное освещение, сложив три простых компонента:
- Ambient: Дает базовый цвет, чтобы в тенях не было черноты.
- Diffuse: Создает объем, показывая, как свет "обтекает" форму объекта.
- Specular: Добавляет блики, показывая, насколько материал глянцевый.
Она так популярна в real-time графике, потому что очень дешева в вычислениях (требует всего несколько векторных операций на пиксель), но при этом дает достаточно убедительный результат.